{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a45ff96-59b0-46a2-9049-f681c8d850a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-1.5B\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2-1.5B\", device_map=\"auto\", torch_dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa80be8e-14b2-409e-9655-4dbaf244290a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0447c70-98fb-4a5b-9881-a56b9bc710ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a373f207-a3d0-48fc-85cc-3189d0ea6b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"Helsinki-NLP/opus-100\", \"en-tr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7d0b92c-b906-429b-a5c1-402a363f337e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 1000000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ecd8e0e0-4713-4860-8935-c206867900ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'translation': {'en': 'I got something.', 'tr': 'Bir şey buldum.'}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "099bc0ed-73d3-42c1-81f5-a259d257aa68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_as_chat(example):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant that translates English to Turkish.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Translate this to Turkish: {example['translation']['en']}\"},\n",
    "        {\"role\": \"assistant\", \"content\": example['translation']['tr']}\n",
    "    ]\n",
    "    \n",
    "    formatted_chat = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "    \n",
    "    return {\"text\": formatted_chat}\n",
    "\n",
    "chat_dataset = dataset.map(format_as_chat, remove_columns=[\"translation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2c7d5b9-ce60-4a93-938b-f6e8f1601f55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '<|im_start|>system\\nYou are a helpful assistant that translates English to Turkish.<|im_end|>\\n<|im_start|>user\\nTranslate this to Turkish: I got something.<|im_end|>\\n<|im_start|>assistant\\nBir şey buldum.<|im_end|>\\n'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "634ff02e-00fc-46c7-bb93-a36cff5d20bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForSeq2Seq\n",
    "from datasets import load_dataset\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "import torch\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    outputs = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        padding=False,  \n",
    "        return_tensors=None\n",
    "    ) \n",
    "    outputs[\"labels\"] = outputs[\"input_ids\"].copy()\n",
    "    \n",
    "    return outputs\n",
    "\n",
    "tokenized_dataset = chat_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"text\"],\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    padding=True, \n",
    "    return_tensors=\"pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21d074a3-82c7-47cf-bf8e-54e38c136939",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1000000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72e48a01-6bc0-4991-a6cc-5a8f4f9225c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful assistant that translates English to Turkish.<|im_end|>\n",
      "<|im_start|>user\n",
      "Translate this to Turkish: I got something.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Bir şey buldum.<|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(tokenized_dataset[\"train\"][0][\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bdeb1acc-aa7e-4fe6-b2f5-1f37d4d523f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tokenized_dataset[\"train\"].shuffle(seed=42).select(range(500))\n",
    "validation_dataset = tokenized_dataset[\"validation\"].shuffle(seed=42).select(range(100))\n",
    "test_dataset = tokenized_dataset[\"test\"].shuffle(seed=42).select(range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3a5779b-52aa-4aa1-b424-c509c6d27da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=8,  # Rank of the update matrices\n",
    "    lora_alpha=32,  # Alpha parameter for LoRA scaling\n",
    "    lora_dropout=0.1,  # Dropout probability for LoRA layers\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],  # Adjust based on model architecture\n",
    "    bias=\"none\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d5c7aca8-3fee-4d02-b59f-169e01e13539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2,179,072 || all params: 1,545,893,376 || trainable%: 0.1410\n"
     ]
    }
   ],
   "source": [
    "# Create LoRA model\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()  # Display percentage of trainable parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b864efd-7c7f-4d00-8e25-730e6ddd7a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/qwen2-tr-en-lora\",\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=2e-4,\n",
    "    num_train_epochs=3,\n",
    "    logging_steps=5,\n",
    "    save_steps=50,\n",
    "    save_total_limit=3,\n",
    "    group_by_length=True, \n",
    "    logging_strategy=\"steps\",\n",
    "    report_to=\"wandb\", \n",
    "    bf16=True,\n",
    "    run_name=\"qwen2-tr-en-lora\",  \n",
    "    label_names=[\"labels\"], \n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=validation_dataset,\n",
    "    data_collator=data_collator,  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9faf22a3-bcc8-479a-9027-96f0b4ca37ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9345f93a-e85d-42d0-9eb3-bcdcb569a14b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "wandb: Currently logged in as: aysenurciftcieee (aysenurciftci) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\aysen\\OneDrive\\Masaüstü\\gpt_scratch\\notebook_proje\\wandb\\run-20250320_142558-gw7l6k5r</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/aysenurciftci/huggingface/runs/gw7l6k5r' target=\"_blank\">qwen2-tr-en-lora</a></strong> to <a href='https://wandb.ai/aysenurciftci/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/aysenurciftci/huggingface' target=\"_blank\">https://wandb.ai/aysenurciftci/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/aysenurciftci/huggingface/runs/gw7l6k5r' target=\"_blank\">https://wandb.ai/aysenurciftci/huggingface/runs/gw7l6k5r</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='93' max='93' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [93/93 06:05, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.127900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.355500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.129700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.354700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.008600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>1.159500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.959700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>1.055800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.335100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>1.146200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.881700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>1.123200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.029900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>1.252000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.127600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.853300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.242800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=93, training_loss=1.1747801585863995, metrics={'train_runtime': 376.5379, 'train_samples_per_second': 3.984, 'train_steps_per_second': 0.247, 'total_flos': 633978505912320.0, 'train_loss': 1.1747801585863995, 'epoch': 2.9206349206349205})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f0e908b6-e3be-4e6d-8ffe-90afa52d742e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./fine_tuned_qwen2-tr-en-lora\\\\tokenizer_config.json',\n",
       " './fine_tuned_qwen2-tr-en-lora\\\\special_tokens_map.json',\n",
       " './fine_tuned_qwen2-tr-en-lora\\\\vocab.json',\n",
       " './fine_tuned_qwen2-tr-en-lora\\\\merges.txt',\n",
       " './fine_tuned_qwen2-tr-en-lora\\\\added_tokens.json',\n",
       " './fine_tuned_qwen2-tr-en-lora\\\\tokenizer.json')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save_model(\"./fine_tuned_qwen2-tr-en-lora\")\n",
    "tokenizer.save_pretrained(\"./fine_tuned_qwen2-tr-en-lora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca10645d-84d0-4584-a886-4f0ca3611123",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# Load the tokenizer and model from your saved directory\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/fine_tuned_qwen2-tr-en-lora\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"/fine_tuned_qwen2-tr-en-lora\",\n",
    "                                             device_map=\"cpu,\n",
    "                                             torch_dtype=torch.float16)\n",
    "\n",
    "# Function to translate English to Turkish\n",
    "def translate_en_to_tr(english_text):\n",
    "    # Format as chat\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant that translates English to Turkish.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Translate this to Turkish: {english_text}\"}\n",
    "    ]\n",
    "    \n",
    "    # Apply chat template\n",
    "    formatted_chat = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(formatted_chat, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # Generate translation\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs.input_ids,\n",
    "            max_new_tokens=128,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "    \n",
    "    # Decode the generated tokens\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract the assistant's response (the translation)\n",
    "    # This depends on the chat template format, you might need to adjust\n",
    "    translation = generated_text.split(\"assistant\")[-1].strip()\n",
    "    \n",
    "    return translation\n",
    "\n",
    "# Test with a few examples from your test dataset\n",
    "for i in range(5):  # Test with first 5 examples\n",
    "    example = test_dataset[i]\n",
    "    original_text = tokenizer.decode(example[\"input_ids\"], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract the original English text\n",
    "    english_text = original_text.split(\"user\")[-1].split(\"assistant\")[0].strip()\n",
    "    if \"Translate this to Turkish:\" in english_text:\n",
    "        english_text = english_text.split(\"Translate this to Turkish:\")[1].strip()\n",
    "    \n",
    "    # Get the expected Turkish translation\n",
    "    expected_translation = original_text.split(\"assistant\")[-1].strip()\n",
    "    \n",
    "    # Get the model's translation\n",
    "    model_translation = translate_en_to_tr(english_text)\n",
    "    \n",
    "    print(f\"English: {english_text}\")\n",
    "    print(f\"Expected: {expected_translation}\")\n",
    "    print(f\"Model output: {model_translation}\")\n",
    "    print(\"-\" * 50)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
